{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is used to parse a single HTML file into plain text and detect valid sentences.\n",
    "\n",
    "Last modified: 07.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import itertools\n",
    "import copy\n",
    "from chempp.article import (\n",
    "    ArticleElementType,\n",
    "    ArticleElement\n",
    ")\n",
    "from chempp.article_constr import search_html_doi_publisher\n",
    "from chempp.table import (\n",
    "    Table,\n",
    "    TableCell,\n",
    "    TableRow,\n",
    "    set_table_style\n",
    ")\n",
    "from chempp.article_constr import ArticleFunctions, check_html_publisher, search_xml_doi_publisher\n",
    "from chempp.section_extr import (\n",
    "    html_table_extract_wiley,\n",
    "    html_table_extract_rsc,\n",
    "    html_table_extract_springer,\n",
    "    html_table_extract_acs,\n",
    "    html_table_extract_elsevier,\n",
    "    xml_table_extract_acs,\n",
    "    xml_table_extract_elsevier,\n",
    "    get_html_table_rows,\n",
    "    get_xml_text_iter,\n",
    "    pop_xml_element_iter\n",
    ")\n",
    "from chempp.constants import *\n",
    "\n",
    "from chemdataextractor.doc import Paragraph\n",
    "from bs4 import BeautifulSoup\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\rop-database-v2\\10.1007@s00289-011-0647-0.html\n"
     ]
    }
   ],
   "source": [
    "folder = r'data/rop-database-v2'\n",
    "# folder = r'data/download/gs'\n",
    "# folder = r'.'\n",
    "file_name = '*10.1007@s00289-011-0647-0*'\n",
    "# file_name = '*10.1021@ma0114887*'  # ACS\n",
    "# file_name = '*10.1016@j.polymer.2005.11.025*'  # ELSEVIER\n",
    "# file_name = '*10.1007@s10965-013-0244-z*'  # springer\n",
    "# file_name = '*10.1039@d0py00155d*'  # RSC\n",
    "# file_name = '*10.1002@pola.22406*'  # wiley\n",
    "# file_name = '*10.1021^slcr940282j*' # ACS HTML\n",
    "# file_name = '*10.1016^slj.chempr.2020.06.003*'  # elsevier HTML\n",
    "suffix = 'html'\n",
    "\n",
    "if suffix == 'html':\n",
    "    path_list = list()\n",
    "    for file_path in glob.glob(os.path.join(folder, file_name + \".html\")):\n",
    "        path_list.append(file_path)\n",
    "    file_path = path_list[0]\n",
    "\n",
    "elif suffix == 'xml':\n",
    "    path_list = list()\n",
    "    for file_path in glob.glob(os.path.join(folder, file_name + \".xml\")):\n",
    "        path_list.append(file_path)\n",
    "    file_path = path_list[0]\n",
    "\n",
    "file_path = os.path.normpath(file_path)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "springer\n"
     ]
    }
   ],
   "source": [
    "publisher_file_dict = {publisher: [] for publisher in SUPPORTED_HTML_PUBLISHERS}\n",
    "\n",
    "if file_path.endswith('html'):\n",
    "    file_path = os.path.normpath(file_path)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "\n",
    "    # get publisher and doi\n",
    "    doi, publisher = search_html_doi_publisher(soup)\n",
    "\n",
    "    if publisher == 'elsevier':\n",
    "        # allow illegal nested <p>\n",
    "        soup = BeautifulSoup(contents, 'html.parser')\n",
    "    elif publisher == 'rsc':\n",
    "        # allow nested <span>\n",
    "        soup = BeautifulSoup(contents, 'html5lib')\n",
    "\n",
    "    article_construct_func = getattr(ArticleFunctions, f'article_construct_html_{publisher}')\n",
    "    article, component_check = article_construct_func(soup=soup, doi=doi)\n",
    "elif file_path.endswith('xml'):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    doi, publisher = search_xml_doi_publisher(root)\n",
    "\n",
    "    article_construct_func = getattr(ArticleFunctions, f'article_construct_xml_{publisher}')\n",
    "    article, component_check = article_construct_func(root=root, doi=doi)\n",
    "print(publisher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load pre-defined article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'results/IE-results-v2.2/rop-database-v2_processed'\n",
    "file_name = '*10.1021&sl;acs.macromol.9b01777*'\n",
    "\n",
    "path_list = list()\n",
    "for file_path in glob.glob(os.path.join(folder, file_name)):\n",
    "    path_list.append(file_path)\n",
    "file_path = path_list[0]\n",
    "\n",
    "article = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in article.paragraphs:\n",
    "    if 'Limonene oxide displays a promising' in para.text:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'c3'\n",
    "article_property_info = get_property_info(article, criterion)\n",
    "\n",
    "for property_info_dict in article_property_info:\n",
    "    property_info_dict['doi'] = article.doi\n",
    "    property_info_dict['file-path'] = os.path.join('html-files', '???')\n",
    "    property_info_dict['criterion'] = criterion if criterion is not None else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Our group recently investigated the ceiling temperature of PLC and observed depolymerization at 60 °C at elevated conversions.',\n",
       "  'sentence-id': 1,\n",
       "  'material': 'PLC',\n",
       "  'property': 'ceiling temperature',\n",
       "  'value': '60 °C',\n",
       "  'type': 'Tc',\n",
       "  'reliability': 0.1,\n",
       "  'doi': '10.1021/acs.macromol.9b01777',\n",
       "  'file-path': 'html-files\\\\???',\n",
       "  'criterion': 'c3'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_property_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored functions; don't delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Don't delete\n",
    "# how the table extraction functions are used\n",
    "\n",
    "## rsc\n",
    "tables = soup.find_all('div', {\"class\": \"rtable__wrapper\"})\n",
    "table_div = tables[0]  # or 1, 2, ...\n",
    "\n",
    "tbl = html_table_extract_rsc(table_div)\n",
    "\n",
    "## wiley\n",
    "tables = soup.find_all('div', {\"class\": \"article-table-content\"})\n",
    "table_div = tables[0]\n",
    "\n",
    "tbl = html_table_extract_wiley(table_div)\n",
    "\n",
    "# springer\n",
    "tables = soup.find_all('div', {\"class\": \"Table\"})\n",
    "table_div = tables[0]\n",
    "\n",
    "tbl = html_table_extract_springer(table_div)\n",
    "\n",
    "## acs---html\n",
    "tables = soup.find_all('div', {\"class\": \"NLM_table-wrap\"})\n",
    "table_div = tables[0]\n",
    "\n",
    "tbl = html_table_extract_acs(table_div)\n",
    "\n",
    "## acs---xml\n",
    "body = root.findall('body')[0]\n",
    "tables = list(body.iter(tag=r'table-wrap'))\n",
    "xml_table = tables[0]\n",
    "\n",
    "tbl = xml_table_extract_acs(xml_table)\n",
    "\n",
    "## elsevier---html\n",
    "tables = soup.find_all('div', {\"class\": \"tables\"})\n",
    "table_div = tables[0]\n",
    "\n",
    "tbl = html_table_extract_elsevier(table_div)\n",
    "\n",
    "## elsevier--xml\n",
    "ori_txt = root.findall(r'{http://www.elsevier.com/xml/svapi/article/dtd}originalText')[0]\n",
    "doc = ori_txt.findall(r'{http://www.elsevier.com/xml/xocs/dtd}doc')[0]\n",
    "\n",
    "tables = list(doc.iter(tag=r'{http://www.elsevier.com/xml/common/dtd}table'))\n",
    "xml_table = tables[0]\n",
    "tbl = xml_table_extract_elsevier(xml_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Don't delete\n",
    "# Save table into HTML\n",
    "\n",
    "soup = BeautifulSoup()\n",
    "head = soup.new_tag('head', style=\"width: 85%; margin:auto auto;\")\n",
    "soup.insert(0, head)\n",
    "title = soup.new_tag('title')\n",
    "head.insert(0, title)\n",
    "title.insert(0, article.title)\n",
    "\n",
    "set_table_style(head)\n",
    "\n",
    "html_body = soup.new_tag('body', style=\"width: 85%; margin:auto auto;\")\n",
    "soup.insert(len(soup), html_body)\n",
    "\n",
    "_ = write_table(table, html_body)\n",
    "\n",
    "with open('test.html', 'w', encoding='utf-8') as outfile:\n",
    "    outfile.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Don't delete\n",
    "print(\"[INFO] extracting information...\")\n",
    "\n",
    "interested_info = dict()\n",
    "\n",
    "for i, paragraph in enumerate(article.sections):\n",
    "    if paragraph.element_type != ArticleElementType.PARAGRAPH:\n",
    "        continue\n",
    "    text = paragraph.text\n",
    "    text = break_number_unit(text, TEMPERATURE_UNITS + ENERGY_UNITS)\n",
    "    paragraph.text = text\n",
    "    article.sections[i] = paragraph\n",
    "\n",
    "    para = Paragraph(text)\n",
    "    sents, sent_ranges = convert_cde_paragraph_to_sentences(para)\n",
    "    valid_sent_ids, _ = locate_valid_sents_heuristic(sents)\n",
    "\n",
    "    if valid_sent_ids:\n",
    "        valid_sents = [sents[idx] for idx in valid_sent_ids]\n",
    "        valid_sent_ranges = [sent_ranges[idx] for idx in valid_sent_ids]\n",
    "        interested_info[i] = valid_sent_ranges\n",
    "\n",
    "for k in interested_info.keys():\n",
    "    interested_info[k] = sort_tuple_by_first_element(interested_info[k])\n",
    "\n",
    "for para_id, sent_spans in interested_info.items():\n",
    "    para = article.sections[para_id].text.strip()\n",
    "    print(sent_spans)\n",
    "    show_box_markup(para, sent_spans)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc8ca3a645f3088b7b5f6533874a55c6025459a1e0343a8c2fa3bcb3488a097"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('xfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}